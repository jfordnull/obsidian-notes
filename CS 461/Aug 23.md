Many current AI systems essentially live in Plato's allegory of the cave. AI lives removed from time, disembodied; Trained on 2D images, flattened projections of reality. Inherent bias in the act of curating training data; We take pictures of things we deem significant. Unsurprising that scope of capabilities is limited when we release AI into the space we expect it to act in.

Is it important that AI use the same methods of intelligence humans use?
- What if humans can't describe in depth how they reach a conclusion (think rationally)?
- What if higher-order intelligence is social, emerges in groups?
	- Individual ants don't demonstrate intelligence, but ant colonies do
- Human intelligence is iterative over generations

Alternatively, an AI system, as an abstraction of human intelligence, could come to a new series of conclusions on how the world works unbiased by human understanding or bodily experience.

Working definition: AI is the science of making machines do things that would require *intelligence* if done by *humans*.

AI may help us better understand our origins and internal processes, better define intelligence, as a mirror. 

"A year spent in Artificial Intelligence is enough to make one believe in God." -Alan Perlis

**Leibniz's Law**

For any two objects X and Y: If X=Y, then X and Y share all the same properties
$$(a=b)\rightarrow\forall F(F(a)\leftarrow\rightarrow F(b))$$
Ship of Theseus Paradox: Is an object the same object after having all its components replaced, one-by-one. Puzzle of material constitution, relationship between an object and its constitutive parts 

AI struggles with continuity and *causal inference*. Problem of durability: AI video and music generation struggles beyond 30-second mark. In part, a failure of memory; Human brain has broad-scale parallelization as memory bank. Where do AI systems fail the Turing Test? In context and continuity; systems lack memory to sustain interaction. 

Assignment moving forward: How would you design a better Turing Test?