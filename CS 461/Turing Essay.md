Alan Turing’s *Computer Machinery and Intelligence* opens with the question, “Can machines think?” After proposing an experimental test for machine thinking, Turing anticipates objections to his own conjecture that by the end of the century, computers will exceed this test, called the “imitation game,” wherein a machine is said to be intelligent if a human interrogator using a textual interface cannot distinguish it from another human being. In this way, Turing centers our linguistic capacity in his attempt to quantify machine intelligence. Present-day LLMs have arguably surpassed the intelligence test as Turing defined it, but few among us seem comfortable ascribing *thinking* to these systems. And so, to improve his criterion for the ChatGPT era, it seems apropos to revisit the objections to Turing’s argument he anticipated. Of these, I find the “Argument from Consciousness” most thought-provoking. 

Turing quotes a Professor Jefferson’s oration in summary of this objection: "Not until a machine can write a sonnet or compose a concerto because of thoughts and emotions felt, and not by the chance fall of symbols, could we agree that machine equals brain – that is, not only write it but know that it had written it.” According to this view, a machine could only be said to think if it could know itself thinking. This brings to mind the Cartesian cogito ergo sum, and Turing is quick to criticize it, at its limit, as the “solipsist point of view.” To my mind, Jefferson’s argument rhymes with John Searle’s parable of the Chinese Room. Even if the machine is able to pair together a sequence of words, phrases, and concepts into a hash chain or lookup table – a “chance fall of symbols” – without internal understanding we cannot ascribe *thought* to the process; Searle would argue it lacks a mind. What we seem to be discussing is the difference between *syntax* and *semantics*. ChatGPT is all syntax. Surely Strong AI, which according to the Carnegie-Mellon school should be based on the same methods of learning and cognition used by humans, would know semantics. 

Turing goes to great lengths in his paper to anonymize input, disregarding internal processes (that is, thinking) in favor of the facade of intelligence. This brings to mind the Ship of Theseus paradox: Are two things the same if they share apparent properties but differ in their constitutive parts? The best rebuttal to Searle’s Chinese Room may supplant that question with its own: What if human intelligence *is* a mapping function? According to some computer scientists, given a large enough database, we could link through all possible logical conjunctions using the co-frequency of tokens – and maybe this encoding reflects the brain’s structure, as a matrix of relations – in other words, as a biological syntax machine. Is the brain a cipher? In what cluster of neurons do semantics reside? What does it even mean to think – and how different is that process from the “chance fall of symbols” in machine intelligence? 

A generic suggestion for improving Turing’s test: Multimodality. The test should involve auditory and visual components, not just text-based interaction. Somehow, it should assess domains of intelligence outside linguistics: spatial, emotional (interpersonal), musical, and bodily (kinesthetic). This requires us to expand our test and our systems into four dimensions. As it stands, AI does not experience z-space or time. We train these systems on flattened projections of our reality in timeless stasis; They live in Plato’s Allegory of the Cave. Spatial-temporal flattening may be, in part, why these systems struggle with continuity, or causal inference – though OpenAI evangelists insist this is just a consequence of insufficient memory. (So give them tithe to buy more data-centers.) A revised Turing test should measure durability in the form of long-term contextual reasoning.  

But what about ethical reasoning? Turing’s test is amoral, an incentive structure for a sociopathic machine intelligence. Or human creativity? The probabilistic methods behind image-generation fail to capture the decision-tree, or the *feeling*, of a human aesthetic object. If our end is Strong AI, we should be concerned with the internal processes Turing chose to disguise – but I am at a loss for a metric. I want AI to explain its decisions to me, to *feel* itself thinking, but I must resist the urge to peer into Pandora’s box any further in a short-form essay.